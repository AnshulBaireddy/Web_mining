{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WE9WGE_m-4J-"
   },
   "source": [
    "# HW 4: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKVEJOmf-4KC"
   },
   "source": [
    "## Q1: Extract data using regular expression (2 points)\n",
    "Suppose you have scraped the text shown below from an online source (https://www.google.com/finance/). \n",
    "Define a `extract` function which:\n",
    "- takes a piece of text (in the format of shown below) as an input\n",
    "- extracts data into a DataFrame with columns 'Ticker','Name','Article','Media','Time','Price',and 'Change' using regular expression\n",
    "- returns the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: spacy in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (3.4.2)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.7)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.0.8)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (21.3)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.10.2)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (61.2.0)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.6.2)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.10.1)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.8)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (0.4.2)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.21.5)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (8.1.5)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.3)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (1.0.9)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.0.10)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (3.3.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (4.64.0)\n",
      "Requirement already satisfied: jinja2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.11.3)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy) (2.4.5)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy) (3.0.4)\n",
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.4)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.7.9)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy) (0.0.3)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy) (2.0.1)\n",
      "Collecting en-core-web-sm==3.4.1\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.4.1/en_core_web_sm-3.4.1-py3-none-any.whl (12.8 MB)\n",
      "\u001b[K     |████████████████████████████████| 12.8 MB 6.4 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: spacy<3.5.0,>=3.4.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from en-core-web-sm==3.4.1) (3.4.2)\n",
      "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.8)\n",
      "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.10 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.10)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.27.1)\n",
      "Requirement already satisfied: setuptools in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (61.2.0)\n",
      "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.9)\n",
      "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3.0)\n",
      "Requirement already satisfied: numpy>=1.15.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.21.5)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.64.0)\n",
      "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.7)\n",
      "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.0.3)\n",
      "Requirement already satisfied: thinc<8.2.0,>=8.1.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.1.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (21.3)\n",
      "Requirement already satisfied: wasabi<1.1.0,>=0.9.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.10.1)\n",
      "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.10.2)\n",
      "Requirement already satisfied: typer<0.5.0,>=0.3.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.4.2)\n",
      "Requirement already satisfied: pathy>=0.3.5 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.6.2)\n",
      "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.4.5)\n",
      "Requirement already satisfied: jinja2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.11.3)\n",
      "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.8)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from packaging>=20.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.0.4)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: smart-open<6.0.0,>=5.2.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from pathy>=0.3.5->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.1.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from pydantic!=1.8,!=1.8.1,<1.11.0,>=1.7.4->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (4.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from requests<3.0.0,>=2.13.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (3.3)\n",
      "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.0.3)\n",
      "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from thinc<8.2.0,>=8.1.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (0.7.9)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from typer<0.5.0,>=0.3.0->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (8.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages (from jinja2->spacy<3.5.0,>=3.4.0->en-core-web-sm==3.4.1) (2.0.1)\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value [] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download en_core_web_sm\n",
    "import en_core_web_sm \n",
    "lp = en_core_web_sm.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:54.060105Z",
     "start_time": "2021-10-19T07:27:50.568579Z"
    },
    "id": "Cq9Xzhnw-4KD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.preprocessing import normalize\n",
    "import re\n",
    "import spacy\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_q1 = '''QQQ\n",
    "Invesco QQQ Trust Series 1\n",
    "Invesco Expands QQQ Innovation Suite to Include Small-Cap ETF\n",
    "PR Newswire • 4 hours ago\n",
    "$265.62\n",
    "1.13%\n",
    "add_circle_outline\n",
    "AAPL\n",
    "Apple Inc\n",
    "Estimating The Fair Value Of Apple Inc. (NASDAQ:AAPL)\n",
    "Yahoo Finance • 4 hours ago\n",
    "$140.41\n",
    "1.50%\n",
    "add_circle_outline\n",
    "TSLA\n",
    "Tesla Inc\n",
    "Could This Tesla Stock Unbalanced Iron Condor Return 23%?\n",
    "Investor's Business Daily • 1 hour ago\n",
    "$218.30\n",
    "0.49%\n",
    "add_circle_outline\n",
    "AMZN\n",
    "Amazon.com, Inc.\n",
    "The Regulators of Facebook, Google and Amazon Also Invest in the Companies' Stocks\n",
    "Wall Street Journal • 2 days ago\n",
    "$110.91\n",
    "1.76%\n",
    "add_circle_outline'''\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28\n"
     ]
    }
   ],
   "source": [
    "print(len(text_q1.split('\\n')))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract(text):\n",
    "    \n",
    "    \n",
    "    ticker = []\n",
    "    for a in range(0,22,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        for item in b:\n",
    "            ticker.append(item)\n",
    "    \n",
    "    name = []\n",
    "    for a in range(1,23,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        for item in b:\n",
    "            name.append(item)\n",
    "    \n",
    "    article = []\n",
    "    for a in range(2,24,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        for item in b:\n",
    "            article.append(item)\n",
    "\n",
    "    media = []\n",
    "    for a in range(3,25,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        c = b[0]\n",
    "        d = re.findall(r\"^(?:[A-Z][^\\s]*\\s?)+\",c)\n",
    "        for item in d:\n",
    "            media.append(item)\n",
    "   \n",
    "    time = []\n",
    "    for a in range(3,25,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        c = b[0]\n",
    "        d = re.findall(r\"[0-9]+ \\w+ \\w+\",c)\n",
    "        for item in d:\n",
    "            time.append(item)\n",
    "        \n",
    "    price = []\n",
    "    for a in range(4,26,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        for item in b:\n",
    "            price.append(item)\n",
    "    \n",
    "    change = []\n",
    "    for a in range(5,27,7):\n",
    "        regex = r\"^(?:[^\\n]*\\n){\"+str(a)+r\"}([^\\n]*)\"\n",
    "        b = re.findall(regex,text)\n",
    "        for item in b:\n",
    "            change.append(item)\n",
    "    \n",
    "    final = {'Ticker': ticker,'Name': name,'Article': article,'Media': media,'Time': time,'Price': price,'Change': change}\n",
    "    df = pd.DataFrame(final)\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Ticker</th>\n",
       "      <th>Name</th>\n",
       "      <th>Article</th>\n",
       "      <th>Media</th>\n",
       "      <th>Time</th>\n",
       "      <th>Price</th>\n",
       "      <th>Change</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>QQQ</td>\n",
       "      <td>Invesco QQQ Trust Series 1</td>\n",
       "      <td>Invesco Expands QQQ Innovation Suite to Includ...</td>\n",
       "      <td>PR Newswire</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>$265.62</td>\n",
       "      <td>1.13%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AAPL</td>\n",
       "      <td>Apple Inc</td>\n",
       "      <td>Estimating The Fair Value Of Apple Inc. (NASDA...</td>\n",
       "      <td>Yahoo Finance</td>\n",
       "      <td>4 hours ago</td>\n",
       "      <td>$140.41</td>\n",
       "      <td>1.50%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>TSLA</td>\n",
       "      <td>Tesla Inc</td>\n",
       "      <td>Could This Tesla Stock Unbalanced Iron Condor ...</td>\n",
       "      <td>Investor's Business Daily</td>\n",
       "      <td>1 hour ago</td>\n",
       "      <td>$218.30</td>\n",
       "      <td>0.49%</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AMZN</td>\n",
       "      <td>Amazon.com, Inc.</td>\n",
       "      <td>The Regulators of Facebook, Google and Amazon ...</td>\n",
       "      <td>Wall Street Journal</td>\n",
       "      <td>2 days ago</td>\n",
       "      <td>$110.91</td>\n",
       "      <td>1.76%</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Ticker                        Name  \\\n",
       "0    QQQ  Invesco QQQ Trust Series 1   \n",
       "1   AAPL                   Apple Inc   \n",
       "2   TSLA                   Tesla Inc   \n",
       "3   AMZN            Amazon.com, Inc.   \n",
       "\n",
       "                                             Article  \\\n",
       "0  Invesco Expands QQQ Innovation Suite to Includ...   \n",
       "1  Estimating The Fair Value Of Apple Inc. (NASDA...   \n",
       "2  Could This Tesla Stock Unbalanced Iron Condor ...   \n",
       "3  The Regulators of Facebook, Google and Amazon ...   \n",
       "\n",
       "                        Media         Time    Price Change  \n",
       "0                PR Newswire   4 hours ago  $265.62  1.13%  \n",
       "1              Yahoo Finance   4 hours ago  $140.41  1.50%  \n",
       "2  Investor's Business Daily    1 hour ago  $218.30  0.49%  \n",
       "3        Wall Street Journal    2 days ago  $110.91  1.76%  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract(text_q1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aab-6OWh-4KI"
   },
   "source": [
    "## Q2: Analyze a document (8 points)\n",
    "\n",
    "When you have a long document, you would like to \n",
    "- Quanitfy how concrete a sentence is\n",
    "- Create a concise summary while preserving it's key information content and overall meaning. Let's implement an `extractive method` based on the concept of TF-IDF. The idea is to identify the key sentences from an article and use them as a summary. \n",
    "\n",
    "\n",
    "Carefully follow the following steps to achieve these two targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "import spacy\n",
    "import string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsnCv2VZ-4KI"
   },
   "source": [
    "### Q2.1. Preprocess the input document (4 points, each step 0.5 point (see below), overall function and logic 2 points)\n",
    "\n",
    "Define a function `proprocess(doc, lemmatized = True, remove_stopword = True, lower_case = True, remove_punctuation = True, pos_tag = False)` \n",
    "- Inputs with four parameters:\n",
    "    - `doc`: an input string (e.g. a document)\n",
    "    - `lemmatized`: an optional boolean parameter to indicate if tokens are lemmatized. The default value is True (i.e. tokens are lemmatized).\n",
    "    - `remove_stopword`: an optional boolean parameter to remove stop words. The default value is True, i.e., remove stop words. \n",
    "    - `remove_punctuation`: optional boolean parameter to remove punctuations. The default values is True, i.e., remove all punctuations.\n",
    "    - `lower_case`: optional boolean parameter to convert all tokens to lower case. The default option is True, i.e., lowercase all tokens.\n",
    "    - `pos_tag`: optional boolean parameter to add a POS tag for each token. The default option is False, i.e., no POS tagging.  \n",
    "       \n",
    "- Split the input `doc` into sentences. Hint, typically, \"\\n\\n\" is used to separate paragraphs. Make sure each sentence does not cross over two paragraphs. (0.5 point)\n",
    "\n",
    "\n",
    "- Tokenize each sentence into unigram tokens and also process the tokens as follows:\n",
    "    - If `lemmatized` is True, lemmatize all unigrams. (0.5 point)\n",
    "    - If `remove_stopword` is set to True, remove all stop words. (0.5 point)\n",
    "    - If `remove_punctuation` is set to True, remove all punctuations. (0.5 point)\n",
    "    - If `lower_case` is set to True, convert all tokens to lower case (0.5 point)\n",
    "    - If `pos_tag` is set to True, find the POS tag for each token and form a tuple for each token, e.g., ('recently', 'ADV'). Either Penn tags or Universal tags are fine. See mapping of these two tagging systems here: https://universaldependencies.org/tagset-conversion/en-penn-uposf.html\n",
    "\n",
    "- Return the original sentence list (`sents`) and also the tokenized (or tagged) sentence list (`tokenized_sents`). \n",
    "   \n",
    "(Hint: you can use [nltk](https://www.nltk.org/api/nltk.html) and [spacy](https://spacy.io/api/token#attributes) package for this task.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/anshulreddy/opt/anaconda3/lib/python3.9/site-packages/spacy/language.py:1895: UserWarning: [W123] Argument disable with value [] is used instead of ['senter'] as specified in the config. Be aware that this might affect other components in your pipeline.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Power of Natural Language Processing \n",
      " ['power', 'natural', 'language', 'processing'] \n",
      "\n",
      "\n",
      "Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones \n",
      " ['recently', 'conventional', 'wisdom', 'ai', 'well', 'human', 'data', 'drive', 'decision', 'make', 'task', 'still', 'inferior', 'human', 'cognitive', 'creative', 'one'] \n",
      "\n",
      "\n",
      " But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do \n",
      " ['past', 'two', 'year', 'language', 'base', 'ai', 'advanced', 'leap', 'bound', 'change', 'common', 'notion', 'technology'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# load test document\n",
    "import re\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# if you downloaded en_core_web_sm use the following:\n",
    "import en_core_web_sm \n",
    "nlp = en_core_web_sm.load()\n",
    "\n",
    "doc = open(\"/Users/anshulreddy/power_of_nlp.txt\", \"r\", encoding='utf-8').read()\n",
    "checker = open(\"/Users/anshulreddy/power_of_nlp.txt\", \"r\", encoding='utf-8').read()\n",
    "\n",
    "def preprocess(doc, lemmatized = True, remove_stopword = True, lower_case = True, remove_punctuation = True, pos_tag = False):\n",
    "    \n",
    "    updatedTokens = []\n",
    "    \n",
    "    lines = doc.splitlines()\n",
    "    filter_lines = list(filter(None, lines))\n",
    "    sents = []\n",
    "\n",
    "    for line in filter_lines:\n",
    "        sents.extend(line.split(\".\"))\n",
    "    \n",
    "    sents = list(filter(None, sents))\n",
    "    \n",
    "    for sent in sents:\n",
    "        updatedTokens.append(word_tokenize(sent))\n",
    "    \n",
    "    if(lower_case):\n",
    "        updatedTokens = lowerCase(updatedTokens)\n",
    "\n",
    "    if(remove_punctuation):\n",
    "        updatedTokens = removePunctuation(updatedTokens)\n",
    "\n",
    "    if(remove_stopword):\n",
    "        updatedTokens = removeStopword(updatedTokens)\n",
    "    \n",
    "    if(lemmatized):\n",
    "        updatedTokens = lematize(updatedTokens)\n",
    "\n",
    "    if(pos_tag):\n",
    "         updatedTokens = posTag(updatedTokens)\n",
    "    \n",
    "    return sents,updatedTokens\n",
    "\n",
    "def lowerCase(tokens):\n",
    "    newTokens = []\n",
    "    \n",
    "    for tokenList in tokens:\n",
    "        temp_tokens = [token.lower() for token in tokenList]\n",
    "        filter_lines = list(filter(None, temp_tokens))\n",
    "        newTokens.append(filter_lines)\n",
    "    \n",
    "    return newTokens\n",
    "\n",
    "def removePunctuation(tokens):\n",
    "    newTokens = []\n",
    "    for tokenList in tokens:\n",
    "        doc = nlp(' '.join(tokenList))\n",
    "        res = []\n",
    "        for word in doc:\n",
    "            if word.is_punct == False:\n",
    "                res.append(word.text)\n",
    "        newTokens.append(res)\n",
    "    return newTokens\n",
    "\n",
    "def removeStopword(tokens):\n",
    "    newTokens = []\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "\n",
    "    for tokenList in tokens:\n",
    "        word_tokens = tokenList\n",
    "        filtered_sentence = [w for w in word_tokens if not w.lower() in stop_words]\n",
    "        filter_lines = list(filter(None, filtered_sentence))\n",
    "        newTokens.append(filter_lines)\n",
    "    return newTokens\n",
    "\n",
    "\n",
    "def lematize(tokens):\n",
    "    newTokens = []\n",
    "    \n",
    "    for tokenList in tokens:\n",
    "        doc = nlp(' '.join(tokenList))\n",
    "        res = []\n",
    "        for word in doc:\n",
    "            res.append(word.lemma_)\n",
    "        newTokens.append(res)\n",
    "    \n",
    "    return newTokens\n",
    "\n",
    "def posTag(tokens):\n",
    "    newTokens = []\n",
    "    \n",
    "    for tokenList in tokens:\n",
    "        doc = nlp(' '.join(tokenList))\n",
    "        res = []\n",
    "        for word in doc:\n",
    "            res.append((word.text,word.pos_))\n",
    "        newTokens.append(res)\n",
    "\n",
    "    return newTokens\n",
    "    \n",
    "sents, lem = preprocess(doc)\n",
    "\n",
    "for i in range(3):\n",
    "    print(sents[i], \"\\n\",lem[i],\"\\n\\n\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Power of Natural Language Processing \n",
      " ['power', 'natural', 'language', 'processing'] \n",
      "\n",
      "\n",
      "Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones \n",
      " ['recently', 'conventional', 'wisdom', 'ai', 'well', 'human', 'data', 'drive', 'decision', 'make', 'task', 'still', 'inferior', 'human', 'cognitive', 'creative', 'one'] \n",
      "\n",
      "\n",
      " But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do \n",
      " ['past', 'two', 'year', 'language', 'base', 'ai', 'advanced', 'leap', 'bound', 'change', 'common', 'notion', 'technology'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sents, lem = preprocess(doc)\n",
    "\n",
    "for i in range(3):\n",
    "    print(sents[i], \"\\n\",lem[i],\"\\n\\n\" )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:56.835401Z",
     "start_time": "2021-10-19T07:27:54.568105Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "drwiHlEb-4KL",
    "outputId": "fe258009-9ff4-43c4-eb71-1a2876e6084d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Power of Natural Language Processing \n",
      " ['power', 'natural', 'language', 'processing'] \n",
      "\n",
      "\n",
      "Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones \n",
      " ['recently', 'conventional', 'wisdom', 'ai', 'well', 'human', 'data', 'drive', 'decision', 'make', 'task', 'still', 'inferior', 'human', 'cognitive', 'creative', 'one'] \n",
      "\n",
      "\n",
      " But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do \n",
      " ['past', 'two', 'year', 'language', 'base', 'ai', 'advanced', 'leap', 'bound', 'change', 'common', 'notion', 'technology'] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with all default options:\n",
    "\n",
    "sents, tokenized_sents = preprocess(doc)\n",
    "\n",
    "for i in range(3):\n",
    "    print(sents[i], \"\\n\",tokenized_sents[i],\"\\n\\n\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Power of Natural Language Processing \n",
      " [('The', 'DET'), ('Power', 'PROPN'), ('of', 'ADP'), ('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'NOUN')] \n",
      "\n",
      "\n",
      "Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones \n",
      " [('Until', 'ADP'), ('recently', 'ADV'), (',', 'PUNCT'), ('the', 'DET'), ('conventional', 'ADJ'), ('wisdom', 'NOUN'), ('was', 'AUX'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('AI', 'PROPN'), ('was', 'AUX'), ('better', 'ADJ'), ('than', 'ADP'), ('humans', 'NOUN'), ('at', 'ADP'), ('data', 'NOUN'), ('-', 'PUNCT'), ('driven', 'VERB'), ('decision', 'NOUN'), ('making', 'VERB'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('still', 'ADV'), ('inferior', 'ADJ'), ('to', 'ADP'), ('humans', 'NOUN'), ('for', 'ADP'), ('cognitive', 'ADJ'), ('and', 'CCONJ'), ('creative', 'ADJ'), ('ones', 'NOUN')] \n",
      "\n",
      "\n",
      " But in the past two years language-based AI has advanced by leaps and bounds, changing common notions of what this technology can do \n",
      " [('But', 'CCONJ'), ('in', 'ADP'), ('the', 'DET'), ('past', 'ADJ'), ('two', 'NUM'), ('years', 'NOUN'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('has', 'AUX'), ('advanced', 'VERB'), ('by', 'ADP'), ('leaps', 'NOUN'), ('and', 'CCONJ'), ('bounds', 'NOUN'), (',', 'PUNCT'), ('changing', 'VERB'), ('common', 'ADJ'), ('notions', 'NOUN'), ('of', 'ADP'), ('what', 'PRON'), ('this', 'DET'), ('technology', 'NOUN'), ('can', 'AUX'), ('do', 'AUX')] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# process text without remove stopwords, punctuation, lowercase, but with pos tagging\n",
    "\n",
    "sents, tokenized_sents = preprocess(doc, lemmatized = False, pos_tag = True, \n",
    "                                    remove_stopword=False, remove_punctuation = False, \n",
    "                                    lower_case = False)\n",
    "\n",
    "for i in range(3):\n",
    "    print(sents[i], \"\\n\",tokenized_sents[i],\"\\n\\n\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q2.2. Quantify sentence concreteness\n",
    "\n",
    "\n",
    "`Concreteness` can increase a message's persuasion. The concreteness can be measured by:\n",
    "- the use of `article` (e.g., a, an, and the), \n",
    "- `adpositions` (e.g., in, at, of, on, etc), and\n",
    "- `quantifiers`, i.e., adjectives before nouns.\n",
    "\n",
    "\n",
    "Define a function `compute_concreteness(tagged_sent)` as follows:\n",
    "- Input argument is `tagged_sent`, a list with (token, pos_tag) tuples as shown above.\n",
    "- Find the three types of tokens: `articles`, `adposition`, and `quantifiers`.\n",
    "- Compute `concereness` score as:  `(the sum of the counts of the three types of tokens)/(total non-punctuation tokens)`.\n",
    "- return the concreteness score, articles, adposition, and quantifiers lists.\n",
    "\n",
    "\n",
    "Find the most concrete and the least concrete sentences from the article. \n",
    "\n",
    "\n",
    "Reference: Peer to Peer Lending: The Relationship Between Language Features, Trustworthiness, and Persuasion Success, https://socialmedialab.sites.stanford.edu/sites/g/files/sbiybj22976/files/media/file/larrimore-jacr-peer-to-peer.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_concreteness(tagged_sent):\n",
    "    \n",
    "    print(tagged_sent)\n",
    "    articles = []\n",
    "    adpositions=[]\n",
    "    quantifier = []\n",
    "    non_punc = []\n",
    "\n",
    "    for sent in tagged_sent:\n",
    "        if sent[1] == 'DET':\n",
    "            articles.append(sent)\n",
    "        elif sent[1] == 'ADP':\n",
    "            adpositions.append(sent)\n",
    "        elif sent[1] == 'ADJ':\n",
    "            quantifier.append(sent)\n",
    "            \n",
    "        if sent[1] != 'PUNCT':\n",
    "            non_punc.append(sent)\n",
    "    \n",
    "    \n",
    "    concreteness = (len(articles) + len(adpositions) + len(quantifier))/len(non_punc)\n",
    "    \n",
    "            \n",
    "    \n",
    "    return concreteness, articles, adpositions,quantifier\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize with pos tag, without change the text much\n",
    "\n",
    "sents, tokenized_sents = preprocess(doc, lemmatized = False, pos_tag = True, \n",
    "                                    remove_stopword=False, remove_punctuation = False, \n",
    "                                    lower_case = False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Until', 'ADP'), ('recently', 'ADV'), (',', 'PUNCT'), ('the', 'DET'), ('conventional', 'ADJ'), ('wisdom', 'NOUN'), ('was', 'AUX'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('AI', 'PROPN'), ('was', 'AUX'), ('better', 'ADJ'), ('than', 'ADP'), ('humans', 'NOUN'), ('at', 'ADP'), ('data', 'NOUN'), ('-', 'PUNCT'), ('driven', 'VERB'), ('decision', 'NOUN'), ('making', 'VERB'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('still', 'ADV'), ('inferior', 'ADJ'), ('to', 'ADP'), ('humans', 'NOUN'), ('for', 'ADP'), ('cognitive', 'ADJ'), ('and', 'CCONJ'), ('creative', 'ADJ'), ('ones', 'NOUN')]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Until recently, the conventional wisdom was that while AI was better than humans at data-driven decision making tasks, it was still inferior to humans for cognitive and creative ones'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0.36666666666666664,\n",
       " [('the', 'DET')],\n",
       " [('Until', 'ADP'),\n",
       "  ('than', 'ADP'),\n",
       "  ('at', 'ADP'),\n",
       "  ('to', 'ADP'),\n",
       "  ('for', 'ADP')],\n",
       " [('conventional', 'ADJ'),\n",
       "  ('better', 'ADJ'),\n",
       "  ('inferior', 'ADJ'),\n",
       "  ('cognitive', 'ADJ'),\n",
       "  ('creative', 'ADJ')])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Test with one sentence\n",
    "\n",
    "idx = 1\n",
    "x = tokenized_sents[idx]\n",
    "concreteness, articles, adpositions,quantifier = compute_concreteness(x)\n",
    "sents[idx]\n",
    "concreteness, articles, adpositions,quantifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DET'), ('Power', 'PROPN'), ('of', 'ADP'), ('Natural', 'PROPN'), ('Language', 'PROPN'), ('Processing', 'NOUN')]\n",
      "[('Until', 'ADP'), ('recently', 'ADV'), (',', 'PUNCT'), ('the', 'DET'), ('conventional', 'ADJ'), ('wisdom', 'NOUN'), ('was', 'AUX'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('AI', 'PROPN'), ('was', 'AUX'), ('better', 'ADJ'), ('than', 'ADP'), ('humans', 'NOUN'), ('at', 'ADP'), ('data', 'NOUN'), ('-', 'PUNCT'), ('driven', 'VERB'), ('decision', 'NOUN'), ('making', 'VERB'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('still', 'ADV'), ('inferior', 'ADJ'), ('to', 'ADP'), ('humans', 'NOUN'), ('for', 'ADP'), ('cognitive', 'ADJ'), ('and', 'CCONJ'), ('creative', 'ADJ'), ('ones', 'NOUN')]\n",
      "[('But', 'CCONJ'), ('in', 'ADP'), ('the', 'DET'), ('past', 'ADJ'), ('two', 'NUM'), ('years', 'NOUN'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('has', 'AUX'), ('advanced', 'VERB'), ('by', 'ADP'), ('leaps', 'NOUN'), ('and', 'CCONJ'), ('bounds', 'NOUN'), (',', 'PUNCT'), ('changing', 'VERB'), ('common', 'ADJ'), ('notions', 'NOUN'), ('of', 'ADP'), ('what', 'PRON'), ('this', 'DET'), ('technology', 'NOUN'), ('can', 'AUX'), ('do', 'AUX')]\n",
      "[('The', 'DET'), ('most', 'ADV'), ('visible', 'ADJ'), ('advances', 'NOUN'), ('have', 'AUX'), ('been', 'AUX'), ('in', 'ADP'), ('what', 'PRON'), ('’', 'PUNCT'), ('s', 'AUX'), ('called', 'VERB'), ('“', 'PUNCT'), ('natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('”', 'PUNCT'), ('(', 'PUNCT'), ('NLP', 'PROPN'), (')', 'PUNCT'), (',', 'PUNCT'), ('the', 'DET'), ('branch', 'NOUN'), ('of', 'ADP'), ('AI', 'PROPN'), ('focused', 'VERB'), ('on', 'ADP'), ('how', 'SCONJ'), ('computers', 'NOUN'), ('can', 'AUX'), ('process', 'VERB'), ('language', 'NOUN'), ('like', 'ADP'), ('humans', 'NOUN'), ('do', 'VERB')]\n",
      "[('It', 'PRON'), ('has', 'AUX'), ('been', 'AUX'), ('used', 'VERB'), ('to', 'PART'), ('write', 'VERB'), ('an', 'DET'), ('article', 'NOUN'), ('for', 'ADP'), ('The', 'DET'), ('Guardian', 'PROPN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('AI', 'PROPN'), ('-', 'PUNCT'), ('authored', 'VERB'), ('blog', 'NOUN'), ('posts', 'NOUN'), ('have', 'AUX'), ('gone', 'VERB'), ('viral', 'ADJ'), ('—', 'PUNCT'), ('feats', 'NOUN'), ('that', 'PRON'), ('weren', 'PROPN'), ('’', 'PUNCT'), ('t', 'PROPN'), ('possible', 'ADJ'), ('a', 'DET'), ('few', 'ADJ'), ('years', 'NOUN'), ('ago', 'ADP')]\n",
      "[('AI', 'PROPN'), ('even', 'ADV'), ('excels', 'NOUN'), ('at', 'ADP'), ('cognitive', 'ADJ'), ('tasks', 'NOUN'), ('like', 'ADP'), ('programming', 'NOUN'), ('where', 'SCONJ'), ('it', 'PRON'), ('is', 'AUX'), ('able', 'ADJ'), ('to', 'PART'), ('generate', 'VERB'), ('programs', 'NOUN'), ('for', 'ADP'), ('simple', 'ADJ'), ('video', 'NOUN'), ('games', 'NOUN'), ('from', 'ADP'), ('human', 'ADJ'), ('instructions', 'NOUN')]\n",
      "[('Yet', 'CCONJ'), ('while', 'SCONJ'), ('these', 'DET'), ('stunts', 'NOUN'), ('may', 'AUX'), ('be', 'AUX'), ('attention', 'NOUN'), ('grabbing', 'VERB'), (',', 'PUNCT'), ('are', 'AUX'), ('they', 'PRON'), ('really', 'ADV'), ('indicative', 'ADJ'), ('of', 'ADP'), ('what', 'PRON'), ('this', 'DET'), ('tech', 'NOUN'), ('can', 'AUX'), ('do', 'AUX'), ('for', 'ADP'), ('businesses', 'NOUN'), ('?', 'PUNCT')]\n",
      "[('What', 'PRON'), ('NLP', 'NOUN'), ('Can', 'AUX'), ('Do', 'VERB')]\n",
      "[('The', 'DET'), ('best', 'ADV'), ('known', 'VERB'), ('natural', 'ADJ'), ('language', 'NOUN'), ('processing', 'NOUN'), ('tool', 'NOUN'), ('is', 'AUX'), ('GPT-3', 'PROPN'), (',', 'PUNCT'), ('from', 'ADP'), ('OpenAI', 'PROPN'), (',', 'PUNCT'), ('which', 'PRON'), ('uses', 'VERB'), ('AI', 'PROPN'), ('and', 'CCONJ'), ('statistics', 'NOUN'), ('to', 'PART'), ('predict', 'VERB'), ('the', 'DET'), ('next', 'ADJ'), ('word', 'NOUN'), ('in', 'ADP'), ('a', 'DET'), ('sentence', 'NOUN'), ('based', 'VERB'), ('on', 'ADP'), ('the', 'DET'), ('preceding', 'VERB'), ('words', 'NOUN')]\n",
      "[('NLP', 'PROPN'), ('practitioners', 'NOUN'), ('call', 'VERB'), ('tools', 'NOUN'), ('like', 'ADP'), ('this', 'DET'), ('“', 'PUNCT'), ('language', 'NOUN'), ('models', 'NOUN'), (',', 'PUNCT'), ('”', 'PUNCT'), ('and', 'CCONJ'), ('they', 'PRON'), ('can', 'AUX'), ('be', 'AUX'), ('used', 'VERB'), ('for', 'ADP'), ('simple', 'ADJ'), ('analytics', 'NOUN'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('such', 'ADJ'), ('as', 'ADP'), ('classifying', 'VERB'), ('documents', 'NOUN'), ('and', 'CCONJ'), ('analyzing', 'VERB'), ('the', 'DET'), ('sentiment', 'NOUN'), ('in', 'ADP'), ('blocks', 'NOUN'), ('of', 'ADP'), ('text', 'NOUN'), (',', 'PUNCT'), ('as', 'ADV'), ('well', 'ADV'), ('as', 'ADP'), ('more', 'ADV'), ('advanced', 'ADJ'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('such', 'ADJ'), ('as', 'ADP'), ('answering', 'VERB'), ('questions', 'NOUN'), ('and', 'CCONJ'), ('summarizing', 'VERB'), ('reports', 'NOUN')]\n",
      "[('Language', 'NOUN'), ('models', 'NOUN'), ('are', 'AUX'), ('already', 'ADV'), ('reshaping', 'VERB'), ('traditional', 'ADJ'), ('text', 'NOUN'), ('analytics', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('GPT-3', 'PROPN'), ('was', 'AUX'), ('an', 'DET'), ('especially', 'ADV'), ('pivotal', 'ADJ'), ('language', 'NOUN'), ('model', 'NOUN'), ('because', 'SCONJ'), (',', 'PUNCT'), ('at', 'ADP'), ('10x', 'NUM'), ('larger', 'ADJ'), ('than', 'ADP'), ('any', 'DET'), ('previous', 'ADJ'), ('model', 'NOUN'), ('upon', 'SCONJ'), ('release', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('was', 'AUX'), ('the', 'DET'), ('first', 'ADJ'), ('large', 'ADJ'), ('language', 'NOUN'), ('model', 'NOUN'), (',', 'PUNCT'), ('which', 'PRON'), ('enabled', 'VERB'), ('it', 'PRON'), ('to', 'PART'), ('perform', 'VERB'), ('even', 'ADV'), ('more', 'ADV'), ('advanced', 'ADJ'), ('tasks', 'NOUN'), ('like', 'ADP'), ('programming', 'NOUN'), ('and', 'CCONJ'), ('solving', 'VERB'), ('high', 'ADJ'), ('school', 'NOUN'), ('–', 'PUNCT'), ('level', 'NOUN'), ('math', 'NOUN'), ('problems', 'NOUN')]\n",
      "[('The', 'DET'), ('latest', 'ADJ'), ('version', 'NOUN'), (',', 'PUNCT'), ('called', 'VERB'), ('InstructGPT', 'NOUN'), (',', 'PUNCT'), ('has', 'AUX'), ('been', 'AUX'), ('fine', 'ADV'), ('-', 'PUNCT'), ('tuned', 'VERB'), ('by', 'ADP'), ('humans', 'NOUN'), ('to', 'PART'), ('generate', 'VERB'), ('responses', 'NOUN'), ('that', 'PRON'), ('are', 'AUX'), ('much', 'ADV'), ('better', 'ADV'), ('aligned', 'VERB'), ('with', 'ADP'), ('human', 'ADJ'), ('values', 'NOUN'), ('and', 'CCONJ'), ('user', 'NOUN'), ('intentions', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('Google', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('latest', 'ADJ'), ('model', 'NOUN'), ('shows', 'VERB'), ('further', 'ADJ'), ('impressive', 'ADJ'), ('breakthroughs', 'NOUN'), ('on', 'ADP'), ('language', 'NOUN'), ('and', 'CCONJ'), ('reasoning', 'NOUN')]\n",
      "[('For', 'ADP'), ('businesses', 'NOUN'), (',', 'PUNCT'), ('the', 'DET'), ('three', 'NUM'), ('areas', 'NOUN'), ('where', 'SCONJ'), ('GPT-3', 'PROPN'), ('has', 'AUX'), ('appeared', 'VERB'), ('most', 'ADV'), ('promising', 'ADJ'), ('are', 'AUX'), ('writing', 'VERB'), (',', 'PUNCT'), ('coding', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('discipline', 'NOUN'), ('-', 'PUNCT'), ('specific', 'ADJ'), ('reasoning', 'NOUN')]\n",
      "[('OpenAI', 'PROPN'), (',', 'PUNCT'), ('the', 'DET'), ('Microsoft', 'PROPN'), ('-', 'PUNCT'), ('funded', 'VERB'), ('creator', 'NOUN'), ('of', 'ADP'), ('GPT-3', 'PROPN'), (',', 'PUNCT'), ('has', 'AUX'), ('developed', 'VERB'), ('a', 'DET'), ('GPT-3', 'PROPN'), ('-', 'PUNCT'), ('based', 'VERB'), ('language', 'NOUN'), ('model', 'NOUN'), ('intended', 'VERB'), ('to', 'PART'), ('act', 'VERB'), ('as', 'ADP'), ('an', 'DET'), ('assistant', 'NOUN'), ('for', 'ADP'), ('programmers', 'NOUN'), ('by', 'ADP'), ('generating', 'VERB'), ('code', 'NOUN'), ('from', 'ADP'), ('natural', 'ADJ'), ('language', 'NOUN'), ('input', 'NOUN')]\n",
      "[('This', 'DET'), ('tool', 'NOUN'), (',', 'PUNCT'), ('Codex', 'PROPN'), (',', 'PUNCT'), ('is', 'AUX'), ('already', 'ADV'), ('powering', 'VERB'), ('products', 'NOUN'), ('like', 'ADP'), ('Copilot', 'PROPN'), ('for', 'ADP'), ('Microsoft', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('subsidiary', 'NOUN'), ('GitHub', 'PROPN'), ('and', 'CCONJ'), ('is', 'AUX'), ('capable', 'ADJ'), ('of', 'ADP'), ('creating', 'VERB'), ('a', 'DET'), ('basic', 'ADJ'), ('video', 'NOUN'), ('game', 'NOUN'), ('simply', 'ADV'), ('by', 'ADP'), ('typing', 'VERB'), ('instructions', 'NOUN')]\n",
      "[('This', 'DET'), ('transformative', 'NOUN'), ('capability', 'NOUN'), ('was', 'AUX'), ('already', 'ADV'), ('expected', 'VERB'), ('to', 'PART'), ('change', 'VERB'), ('the', 'DET'), ('nature', 'NOUN'), ('of', 'ADP'), ('how', 'SCONJ'), ('programmers', 'NOUN'), ('do', 'AUX'), ('their', 'PRON'), ('jobs', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('models', 'NOUN'), ('continue', 'VERB'), ('to', 'PART'), ('improve', 'VERB'), ('—', 'PUNCT'), ('the', 'DET'), ('latest', 'ADJ'), ('from', 'ADP'), ('Google', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('DeepMind', 'PROPN'), ('AI', 'PROPN'), ('lab', 'NOUN'), (',', 'PUNCT'), ('for', 'ADP'), ('example', 'NOUN'), (',', 'PUNCT'), ('demonstrates', 'VERB'), ('the', 'DET'), ('critical', 'ADJ'), ('thinking', 'NOUN'), ('and', 'CCONJ'), ('logic', 'NOUN'), ('skills', 'NOUN'), ('necessary', 'ADJ'), ('to', 'PART'), ('outperform', 'VERB'), ('most', 'ADJ'), ('humans', 'NOUN'), ('in', 'ADP'), ('programming', 'NOUN'), ('competitions', 'NOUN')]\n",
      "[('Models', 'NOUN'), ('like', 'ADP'), ('GPT-3', 'PROPN'), ('are', 'AUX'), ('considered', 'VERB'), ('to', 'PART'), ('be', 'AUX'), ('foundation', 'NOUN'), ('models', 'NOUN'), ('—', 'PUNCT'), ('an', 'DET'), ('emerging', 'VERB'), ('AI', 'PROPN'), ('research', 'NOUN'), ('area', 'NOUN'), ('—', 'PUNCT'), ('which', 'PRON'), ('also', 'ADV'), ('work', 'VERB'), ('for', 'ADP'), ('other', 'ADJ'), ('types', 'NOUN'), ('of', 'ADP'), ('data', 'NOUN'), ('such', 'ADJ'), ('as', 'ADP'), ('images', 'NOUN'), ('and', 'CCONJ'), ('video', 'NOUN')]\n",
      "[('Foundation', 'PROPN'), ('models', 'NOUN'), ('can', 'AUX'), ('even', 'ADV'), ('be', 'AUX'), ('trained', 'VERB'), ('on', 'ADP'), ('multiple', 'ADJ'), ('forms', 'NOUN'), ('of', 'ADP'), ('data', 'NOUN'), ('at', 'ADP'), ('the', 'DET'), ('same', 'ADJ'), ('time', 'NOUN'), (',', 'PUNCT'), ('like', 'ADP'), ('OpenAI', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('DALL·E', 'PROPN'), ('2', 'NUM'), (',', 'PUNCT'), ('which', 'PRON'), ('is', 'AUX'), ('trained', 'VERB'), ('on', 'ADP'), ('language', 'NOUN'), ('and', 'CCONJ'), ('images', 'NOUN'), ('to', 'PART'), ('generate', 'VERB'), ('high', 'ADJ'), ('-', 'PUNCT'), ('resolution', 'NOUN'), ('renderings', 'NOUN'), ('of', 'ADP'), ('imaginary', 'ADJ'), ('scenes', 'NOUN'), ('or', 'CCONJ'), ('objects', 'VERB'), ('simply', 'ADV'), ('from', 'ADP'), ('text', 'NOUN'), ('prompts', 'NOUN')]\n",
      "[('Due', 'ADP'), ('to', 'ADP'), ('their', 'PRON'), ('potential', 'NOUN'), ('to', 'PART'), ('transform', 'VERB'), ('the', 'DET'), ('nature', 'NOUN'), ('of', 'ADP'), ('cognitive', 'ADJ'), ('work', 'NOUN'), (',', 'PUNCT'), ('economists', 'NOUN'), ('expect', 'VERB'), ('that', 'SCONJ'), ('foundation', 'NOUN'), ('models', 'NOUN'), ('may', 'AUX'), ('affect', 'VERB'), ('every', 'DET'), ('part', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('economy', 'NOUN'), ('and', 'CCONJ'), ('could', 'AUX'), ('lead', 'VERB'), ('to', 'ADP'), ('increases', 'NOUN'), ('in', 'ADP'), ('economic', 'ADJ'), ('growth', 'NOUN'), ('similar', 'ADJ'), ('to', 'ADP'), ('the', 'DET'), ('industrial', 'ADJ'), ('revolution', 'NOUN')]\n",
      "[('A', 'DET'), ('Language', 'NOUN'), ('-', 'PUNCT'), ('Based', 'VERB'), ('AI', 'PROPN'), ('Research', 'PROPN'), ('Assistant', 'PROPN')]\n",
      "[('In', 'ADP'), ('my', 'PRON'), ('own', 'ADJ'), ('work', 'NOUN'), (',', 'PUNCT'), ('I', 'PRON'), ('’', 'AUX'), ('ve', 'AUX'), ('been', 'AUX'), ('looking', 'VERB'), ('at', 'ADP'), ('how', 'SCONJ'), ('GPT-3', 'PROPN'), ('-', 'PUNCT'), ('based', 'VERB'), ('tools', 'NOUN'), ('can', 'AUX'), ('assist', 'VERB'), ('researchers', 'NOUN'), ('in', 'ADP'), ('the', 'DET'), ('research', 'NOUN'), ('process', 'NOUN')]\n",
      "[('I', 'PRON'), ('am', 'AUX'), ('currently', 'ADV'), ('working', 'VERB'), ('with', 'ADP'), ('Ought', 'PROPN'), (',', 'PUNCT'), ('a', 'DET'), ('San', 'PROPN'), ('Francisco', 'PROPN'), ('company', 'NOUN'), ('developing', 'VERB'), ('an', 'DET'), ('open', 'ADV'), ('-', 'PUNCT'), ('ended', 'ADJ'), ('reasoning', 'NOUN'), ('tool', 'NOUN'), ('(', 'PUNCT'), ('called', 'VERB'), ('Elicit', 'PROPN'), (')', 'PUNCT'), ('that', 'PRON'), ('is', 'AUX'), ('intended', 'VERB'), ('to', 'PART'), ('help', 'VERB'), ('researchers', 'NOUN'), ('answer', 'VERB'), ('questions', 'NOUN'), ('in', 'ADP'), ('minutes', 'NOUN'), ('or', 'CCONJ'), ('hours', 'NOUN'), ('instead', 'ADV'), ('of', 'ADP'), ('weeks', 'NOUN'), ('or', 'CCONJ'), ('months', 'NOUN')]\n",
      "[('Elicit', 'PROPN'), ('is', 'AUX'), ('designed', 'VERB'), ('for', 'ADP'), ('a', 'DET'), ('growing', 'VERB'), ('number', 'NOUN'), ('of', 'ADP'), ('specific', 'ADJ'), ('tasks', 'NOUN'), ('relevant', 'ADJ'), ('to', 'ADP'), ('research', 'NOUN'), (',', 'PUNCT'), ('like', 'ADP'), ('summarization', 'NOUN'), (',', 'PUNCT'), ('data', 'NOUN'), ('labeling', 'NOUN'), (',', 'PUNCT'), ('rephrasing', 'NOUN'), (',', 'PUNCT'), ('brainstorming', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('literature', 'NOUN'), ('reviews', 'NOUN')]\n",
      "[('I', 'PRON'), ('’', 'AUX'), ('ve', 'AUX'), ('found', 'VERB'), ('—', 'PUNCT'), ('not', 'PART'), ('surprisingly', 'ADV'), ('—', 'PUNCT'), ('that', 'SCONJ'), ('Elicit', 'PROPN'), ('works', 'VERB'), ('better', 'ADV'), ('for', 'ADP'), ('some', 'DET'), ('tasks', 'NOUN'), ('than', 'ADP'), ('others', 'NOUN')]\n",
      "[('Tasks', 'NOUN'), ('like', 'ADP'), ('data', 'NOUN'), ('labeling', 'NOUN'), ('and', 'CCONJ'), ('summarization', 'NOUN'), ('are', 'AUX'), ('still', 'ADV'), ('rough', 'ADJ'), ('around', 'ADP'), ('the', 'DET'), ('edges', 'NOUN'), (',', 'PUNCT'), ('with', 'ADP'), ('noisy', 'ADJ'), ('results', 'NOUN'), ('and', 'CCONJ'), ('spotty', 'ADJ'), ('accuracy', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('research', 'NOUN'), ('from', 'ADP'), ('Ought', 'PROPN'), ('and', 'CCONJ'), ('research', 'VERB'), ('from', 'ADP'), ('OpenAI', 'PROPN'), ('shows', 'VERB'), ('promise', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('future', 'NOUN')]\n",
      "[('For', 'ADP'), ('example', 'NOUN'), (',', 'PUNCT'), ('the', 'DET'), ('rephrase', 'NOUN'), ('task', 'NOUN'), ('is', 'AUX'), ('useful', 'ADJ'), ('for', 'ADP'), ('writing', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('the', 'DET'), ('lack', 'NOUN'), ('of', 'ADP'), ('integration', 'NOUN'), ('with', 'ADP'), ('word', 'NOUN'), ('processing', 'NOUN'), ('apps', 'NOUN'), ('renders', 'VERB'), ('it', 'PRON'), ('impractical', 'ADJ'), ('for', 'ADP'), ('now', 'ADV')]\n",
      "[('Brainstorming', 'VERB'), ('tasks', 'NOUN'), ('are', 'AUX'), ('great', 'ADJ'), ('for', 'ADP'), ('generating', 'VERB'), ('ideas', 'NOUN'), ('or', 'CCONJ'), ('identifying', 'VERB'), ('overlooked', 'VERB'), ('topics', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('despite', 'SCONJ'), ('the', 'DET'), ('noisy', 'ADJ'), ('results', 'NOUN'), ('and', 'CCONJ'), ('barriers', 'NOUN'), ('to', 'ADP'), ('adoption', 'NOUN'), (',', 'PUNCT'), ('they', 'PRON'), ('are', 'AUX'), ('currently', 'ADV'), ('valuable', 'ADJ'), ('for', 'ADP'), ('a', 'DET'), ('variety', 'NOUN'), ('of', 'ADP'), ('situations', 'NOUN')]\n",
      "[('Yet', 'CCONJ'), (',', 'PUNCT'), ('of', 'ADP'), ('all', 'DET'), ('the', 'DET'), ('tasks', 'NOUN'), ('Elicit', 'PROPN'), ('offers', 'VERB'), (',', 'PUNCT'), ('I', 'PRON'), ('find', 'VERB'), ('the', 'DET'), ('literature', 'NOUN'), ('review', 'NOUN'), ('the', 'DET'), ('most', 'ADV'), ('useful', 'ADJ')]\n",
      "[('Because', 'SCONJ'), ('Elicit', 'PROPN'), ('is', 'AUX'), ('an', 'DET'), ('AI', 'PROPN'), ('research', 'NOUN'), ('assistant', 'NOUN'), (',', 'PUNCT'), ('this', 'PRON'), ('is', 'AUX'), ('sort', 'ADV'), ('of', 'ADP'), ('its', 'PRON'), ('bread', 'NOUN'), ('-', 'PUNCT'), ('and', 'CCONJ'), ('-', 'PUNCT'), ('butter', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('when', 'SCONJ'), ('I', 'PRON'), ('need', 'VERB'), ('to', 'PART'), ('start', 'VERB'), ('digging', 'VERB'), ('into', 'ADP'), ('a', 'DET'), ('new', 'ADJ'), ('research', 'NOUN'), ('topic', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('has', 'AUX'), ('become', 'VERB'), ('my', 'PRON'), ('go', 'VERB'), ('-', 'PUNCT'), ('to', 'PART'), ('resource', 'VERB')]\n",
      "[('All', 'PRON'), ('of', 'ADP'), ('this', 'PRON'), ('is', 'AUX'), ('changing', 'VERB'), ('how', 'SCONJ'), ('I', 'PRON'), ('work', 'VERB')]\n",
      "[('I', 'PRON'), ('spend', 'VERB'), ('much', 'ADV'), ('less', 'ADJ'), ('time', 'NOUN'), ('trying', 'VERB'), ('to', 'PART'), ('find', 'VERB'), ('existing', 'VERB'), ('content', 'NOUN'), ('relevant', 'ADJ'), ('to', 'ADP'), ('my', 'PRON'), ('research', 'NOUN'), ('questions', 'NOUN'), ('because', 'SCONJ'), ('its', 'PRON'), ('results', 'NOUN'), ('are', 'AUX'), ('more', 'ADV'), ('applicable', 'ADJ'), ('than', 'ADP'), ('other', 'ADJ'), (',', 'PUNCT'), ('more', 'ADV'), ('traditional', 'ADJ'), ('interfaces', 'NOUN'), ('for', 'ADP'), ('academic', 'ADJ'), ('search', 'NOUN'), ('like', 'ADP'), ('Google', 'PROPN'), ('Scholar', 'PROPN')]\n",
      "[('I', 'PRON'), ('am', 'AUX'), ('also', 'ADV'), ('beginning', 'VERB'), ('to', 'PART'), ('integrate', 'VERB'), ('brainstorming', 'NOUN'), ('tasks', 'NOUN'), ('into', 'ADP'), ('my', 'PRON'), ('work', 'NOUN'), ('as', 'ADV'), ('well', 'ADV'), (',', 'PUNCT'), ('and', 'CCONJ'), ('my', 'PRON'), ('experience', 'NOUN'), ('with', 'ADP'), ('these', 'DET'), ('tools', 'NOUN'), ('has', 'AUX'), ('inspired', 'VERB'), ('my', 'PRON'), ('latest', 'ADJ'), ('research', 'NOUN'), (',', 'PUNCT'), ('which', 'PRON'), ('seeks', 'VERB'), ('to', 'PART'), ('utilize', 'VERB'), ('foundation', 'NOUN'), ('models', 'NOUN'), ('for', 'ADP'), ('supporting', 'VERB'), ('strategic', 'ADJ'), ('planning', 'NOUN')]\n",
      "[('How', 'SCONJ'), ('Can', 'AUX'), ('Organizations', 'NOUN'), ('Prepare', 'VERB'), ('for', 'ADP'), ('the', 'DET'), ('Future', 'PROPN'), ('?', 'PUNCT')]\n",
      "[('Identify', 'VERB'), ('your', 'PRON'), ('text', 'NOUN'), ('data', 'NOUN'), ('assets', 'NOUN'), ('and', 'CCONJ'), ('determine', 'VERB'), ('how', 'SCONJ'), ('the', 'DET'), ('latest', 'ADJ'), ('techniques', 'NOUN'), ('can', 'AUX'), ('be', 'AUX'), ('leveraged', 'VERB'), ('to', 'PART'), ('add', 'VERB'), ('value', 'NOUN'), ('for', 'ADP'), ('your', 'PRON'), ('firm', 'NOUN')]\n",
      "[('You', 'PRON'), ('are', 'AUX'), ('certainly', 'ADV'), ('aware', 'ADJ'), ('of', 'ADP'), ('the', 'DET'), ('value', 'NOUN'), ('of', 'ADP'), ('data', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('you', 'PRON'), ('still', 'ADV'), ('may', 'AUX'), ('be', 'AUX'), ('overlooking', 'VERB'), ('some', 'DET'), ('essential', 'ADJ'), ('data', 'NOUN'), ('assets', 'NOUN'), ('if', 'SCONJ'), ('you', 'PRON'), ('are', 'AUX'), ('not', 'PART'), ('utilizing', 'VERB'), ('text', 'NOUN'), ('analytics', 'NOUN'), ('and', 'CCONJ'), ('NLP', 'PROPN'), ('throughout', 'ADP'), ('your', 'PRON'), ('organization', 'NOUN')]\n",
      "[('Text', 'PROPN'), ('data', 'NOUN'), ('is', 'AUX'), ('certainly', 'ADV'), ('valuable', 'ADJ'), ('for', 'ADP'), ('customer', 'NOUN'), ('experience', 'NOUN'), ('management', 'NOUN'), ('and', 'CCONJ'), ('understanding', 'VERB'), ('the', 'DET'), ('voice', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('customer', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('think', 'VERB'), ('about', 'ADP'), ('other', 'ADJ'), ('text', 'NOUN'), ('data', 'NOUN'), ('assets', 'NOUN'), ('in', 'ADP'), ('your', 'PRON'), ('organization', 'NOUN'), (':', 'PUNCT'), ('emails', 'NOUN'), (',', 'PUNCT'), ('analysts', 'NOUN'), ('’', 'PART'), ('reports', 'NOUN'), (',', 'PUNCT'), ('contracts', 'NOUN'), (',', 'PUNCT'), ('press', 'NOUN'), ('releases', 'NOUN'), (',', 'PUNCT'), ('archives', 'NOUN'), ('—', 'PUNCT'), ('even', 'ADV'), ('meetings', 'NOUN'), ('and', 'CCONJ'), ('phone', 'NOUN'), ('calls', 'NOUN'), ('can', 'AUX'), ('be', 'AUX'), ('transcribed', 'VERB')]\n",
      "[('There', 'PRON'), ('is', 'VERB'), ('so', 'ADV'), ('much', 'ADJ'), ('text', 'NOUN'), ('data', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('you', 'PRON'), ('don', 'VERB'), ('’', 'PUNCT'), ('t', 'NOUN'), ('need', 'VERB'), ('advanced', 'ADJ'), ('models', 'NOUN'), ('like', 'ADP'), ('GPT-3', 'PROPN'), ('to', 'PART'), ('extract', 'VERB'), ('its', 'PRON'), ('value', 'NOUN')]\n",
      "[('Hugging', 'PROPN'), ('Face', 'PROPN'), (',', 'PUNCT'), ('an', 'DET'), ('NLP', 'NOUN'), ('startup', 'NOUN'), (',', 'PUNCT'), ('recently', 'ADV'), ('released', 'VERB'), ('AutoNLP', 'NOUN'), (',', 'PUNCT'), ('a', 'DET'), ('new', 'ADJ'), ('tool', 'NOUN'), ('that', 'PRON'), ('automates', 'VERB'), ('training', 'NOUN'), ('models', 'NOUN'), ('for', 'ADP'), ('standard', 'ADJ'), ('text', 'NOUN'), ('analytics', 'NOUN'), ('tasks', 'NOUN'), ('by', 'ADP'), ('simply', 'ADV'), ('uploading', 'VERB'), ('your', 'PRON'), ('data', 'NOUN'), ('to', 'ADP'), ('the', 'DET'), ('platform', 'NOUN')]\n",
      "[('The', 'DET'), ('data', 'NOUN'), ('still', 'ADV'), ('needs', 'VERB'), ('labels', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('far', 'ADV'), ('fewer', 'ADJ'), ('than', 'ADP'), ('in', 'ADP'), ('other', 'ADJ'), ('applications', 'NOUN')]\n",
      "[('Because', 'SCONJ'), ('many', 'ADJ'), ('firms', 'NOUN'), ('have', 'AUX'), ('made', 'VERB'), ('ambitious', 'ADJ'), ('bets', 'NOUN'), ('on', 'ADP'), ('AI', 'PROPN'), ('only', 'ADV'), ('to', 'PART'), ('struggle', 'VERB'), ('to', 'PART'), ('drive', 'VERB'), ('value', 'NOUN'), ('into', 'ADP'), ('the', 'DET'), ('core', 'NOUN'), ('business', 'NOUN'), (',', 'PUNCT'), ('remain', 'VERB'), ('cautious', 'ADJ'), ('to', 'PART'), ('not', 'PART'), ('be', 'AUX'), ('overzealous', 'ADJ')]\n",
      "[('This', 'PRON'), ('can', 'AUX'), ('be', 'AUX'), ('a', 'DET'), ('good', 'ADJ'), ('first', 'ADJ'), ('step', 'NOUN'), ('that', 'PRON'), ('your', 'PRON'), ('existing', 'VERB'), ('machine', 'NOUN'), ('learning', 'NOUN'), ('engineers', 'NOUN'), ('—', 'PUNCT'), ('or', 'CCONJ'), ('even', 'ADV'), ('talented', 'ADJ'), ('data', 'NOUN'), ('scientists', 'NOUN'), ('—', 'PUNCT'), ('can', 'AUX'), ('manage', 'VERB')]\n",
      "[('To', 'PART'), ('take', 'VERB'), ('the', 'DET'), ('next', 'ADJ'), ('step', 'NOUN'), (',', 'PUNCT'), ('again', 'ADV'), (',', 'PUNCT'), ('identify', 'VERB'), ('your', 'PRON'), ('data', 'NOUN'), ('assets', 'NOUN')]\n",
      "[('Many', 'ADJ'), ('sectors', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('even', 'ADV'), ('divisions', 'NOUN'), ('within', 'ADP'), ('your', 'PRON'), ('organization', 'NOUN'), (',', 'PUNCT'), ('use', 'VERB'), ('highly', 'ADV'), ('specialized', 'ADJ'), ('vocabularies', 'NOUN')]\n",
      "[('Through', 'ADP'), ('a', 'DET'), ('combination', 'NOUN'), ('of', 'ADP'), ('your', 'PRON'), ('data', 'NOUN'), ('assets', 'NOUN'), ('and', 'CCONJ'), ('open', 'ADJ'), ('datasets', 'NOUN'), (',', 'PUNCT'), ('train', 'VERB'), ('a', 'DET'), ('model', 'NOUN'), ('for', 'ADP'), ('the', 'DET'), ('needs', 'NOUN'), ('of', 'ADP'), ('specific', 'ADJ'), ('sectors', 'NOUN'), ('or', 'CCONJ'), ('divisions', 'NOUN')]\n",
      "[('Think', 'VERB'), ('of', 'ADP'), ('finance', 'NOUN')]\n",
      "[('You', 'PRON'), ('do', 'AUX'), ('not', 'PART'), ('want', 'VERB'), ('a', 'DET'), ('model', 'NOUN'), ('specialized', 'VERB'), ('in', 'ADP'), ('finance', 'NOUN')]\n",
      "[('You', 'PRON'), ('want', 'VERB'), ('a', 'DET'), ('model', 'NOUN'), ('customized', 'VERB'), ('for', 'ADP'), ('commercial', 'ADJ'), ('banking', 'NOUN'), (',', 'PUNCT'), ('or', 'CCONJ'), ('for', 'ADP'), ('capital', 'NOUN'), ('markets', 'NOUN')]\n",
      "[('And', 'CCONJ'), ('data', 'NOUN'), ('is', 'AUX'), ('critical', 'ADJ'), (',', 'PUNCT'), ('but', 'CCONJ'), ('now', 'ADV'), ('it', 'PRON'), ('is', 'AUX'), ('unlabeled', 'ADJ'), ('data', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('the', 'PRON'), ('more', 'ADV'), ('the', 'DET'), ('better', 'ADJ')]\n",
      "[('Specialized', 'ADJ'), ('models', 'NOUN'), ('like', 'ADP'), ('this', 'PRON'), ('can', 'AUX'), ('unlock', 'VERB'), ('untold', 'ADJ'), ('value', 'NOUN'), ('for', 'ADP'), ('your', 'PRON'), ('firm', 'NOUN')]\n",
      "[('Understand', 'VERB'), ('how', 'SCONJ'), ('you', 'PRON'), ('might', 'AUX'), ('leverage', 'VERB'), ('AI', 'PROPN'), ('-', 'PUNCT'), ('based', 'VERB'), ('language', 'NOUN'), ('technologies', 'NOUN'), ('to', 'PART'), ('make', 'VERB'), ('better', 'ADJ'), ('decisions', 'NOUN'), ('or', 'CCONJ'), ('reorganize', 'VERB'), ('your', 'PRON'), ('skilled', 'ADJ'), ('labor', 'NOUN')]\n",
      "[('Language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('won', 'VERB'), ('’', 'PUNCT'), ('t', 'NOUN'), ('replace', 'VERB'), ('jobs', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('it', 'PRON'), ('will', 'AUX'), ('automate', 'VERB'), ('many', 'ADJ'), ('tasks', 'NOUN'), (',', 'PUNCT'), ('even', 'ADV'), ('for', 'ADP'), ('decision', 'NOUN'), ('makers', 'NOUN')]\n",
      "[('Startups', 'NOUN'), ('like', 'ADP'), ('Verneek', 'PROPN'), ('are', 'AUX'), ('creating', 'VERB'), ('Elicit', 'ADJ'), ('-', 'PUNCT'), ('like', 'ADJ'), ('tools', 'NOUN'), ('to', 'PART'), ('enable', 'VERB'), ('everyone', 'PRON'), ('to', 'PART'), ('make', 'VERB'), ('data', 'NOUN'), ('-', 'PUNCT'), ('informed', 'VERB'), ('decisions', 'NOUN')]\n",
      "[('These', 'DET'), ('new', 'ADJ'), ('tools', 'NOUN'), ('will', 'AUX'), ('transcend', 'VERB'), ('traditional', 'ADJ'), ('business', 'NOUN'), ('intelligence', 'NOUN'), ('and', 'CCONJ'), ('will', 'AUX'), ('transform', 'VERB'), ('the', 'DET'), ('nature', 'NOUN'), ('of', 'ADP'), ('many', 'ADJ'), ('roles', 'NOUN'), ('in', 'ADP'), ('organizations', 'NOUN'), ('—', 'PUNCT'), ('programmers', 'NOUN'), ('are', 'AUX'), ('just', 'ADV'), ('the', 'DET'), ('beginning', 'NOUN')]\n",
      "[('You', 'PRON'), ('need', 'VERB'), ('to', 'PART'), ('start', 'VERB'), ('understanding', 'VERB'), ('how', 'SCONJ'), ('these', 'DET'), ('technologies', 'NOUN'), ('can', 'AUX'), ('be', 'AUX'), ('used', 'VERB'), ('to', 'PART'), ('reorganize', 'VERB'), ('your', 'PRON'), ('skilled', 'ADJ'), ('labor', 'NOUN')]\n",
      "[('The', 'DET'), ('next', 'ADJ'), ('generation', 'NOUN'), ('of', 'ADP'), ('tools', 'NOUN'), ('like', 'ADP'), ('OpenAI', 'PROPN'), ('’', 'PUNCT'), ('s', 'PRON'), ('Codex', 'PROPN'), ('will', 'AUX'), ('lead', 'VERB'), ('to', 'ADP'), ('more', 'ADJ'), ('productive', 'ADJ'), ('programmers', 'NOUN'), (',', 'PUNCT'), ('which', 'PRON'), ('likely', 'ADV'), ('means', 'VERB'), ('fewer', 'ADJ'), ('dedicated', 'ADJ'), ('programmers', 'NOUN'), ('and', 'CCONJ'), ('more', 'ADJ'), ('employees', 'NOUN'), ('with', 'ADP'), ('modest', 'ADJ'), ('programming', 'NOUN'), ('skills', 'NOUN'), ('using', 'VERB'), ('them', 'PRON'), ('for', 'ADP'), ('an', 'DET'), ('increasing', 'VERB'), ('number', 'NOUN'), ('of', 'ADP'), ('more', 'ADV'), ('complex', 'ADJ'), ('tasks', 'NOUN')]\n",
      "[('This', 'PRON'), ('may', 'AUX'), ('not', 'PART'), ('be', 'AUX'), ('true', 'ADJ'), ('for', 'ADP'), ('all', 'DET'), ('software', 'NOUN'), ('developers', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('it', 'PRON'), ('has', 'VERB'), ('significant', 'ADJ'), ('implications', 'NOUN'), ('for', 'ADP'), ('tasks', 'NOUN'), ('like', 'ADP'), ('data', 'NOUN'), ('processing', 'NOUN'), ('and', 'CCONJ'), ('web', 'NOUN'), ('development', 'NOUN')]\n",
      "[('Begin', 'VERB'), ('incorporating', 'VERB'), ('new', 'ADJ'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('tools', 'NOUN'), ('for', 'ADP'), ('a', 'DET'), ('variety', 'NOUN'), ('of', 'ADP'), ('tasks', 'NOUN'), ('to', 'PART'), ('better', 'ADV'), ('understand', 'VERB'), ('their', 'PRON'), ('capabilities', 'NOUN')]\n",
      "[('Right', 'ADV'), ('now', 'ADV'), ('tools', 'NOUN'), ('like', 'ADP'), ('Elicit', 'PROPN'), ('are', 'AUX'), ('just', 'ADV'), ('emerging', 'VERB'), (',', 'PUNCT'), ('but', 'CCONJ'), ('they', 'PRON'), ('can', 'AUX'), ('already', 'ADV'), ('be', 'AUX'), ('useful', 'ADJ'), ('in', 'ADP'), ('surprising', 'ADJ'), ('ways', 'NOUN')]\n",
      "[('In', 'ADP'), ('fact', 'NOUN'), (',', 'PUNCT'), ('the', 'DET'), ('previous', 'ADJ'), ('suggestion', 'NOUN'), ('was', 'AUX'), ('inspired', 'VERB'), ('by', 'ADP'), ('one', 'NUM'), ('of', 'ADP'), ('Elicit', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('brainstorming', 'NOUN'), ('tasks', 'NOUN'), ('conditioned', 'VERB'), ('on', 'ADP'), ('my', 'PRON'), ('other', 'ADJ'), ('three', 'NUM'), ('suggestions', 'NOUN')]\n",
      "[('The', 'DET'), ('original', 'ADJ'), ('suggestion', 'NOUN'), ('itself', 'PRON'), ('wasn', 'NOUN'), ('’', 'PUNCT'), ('t', 'NOUN'), ('perfect', 'ADJ'), (',', 'PUNCT'), ('but', 'CCONJ'), ('it', 'PRON'), ('reminded', 'VERB'), ('me', 'PRON'), ('of', 'ADP'), ('some', 'DET'), ('critical', 'ADJ'), ('topics', 'NOUN'), ('that', 'PRON'), ('I', 'PRON'), ('had', 'AUX'), ('overlooked', 'VERB'), (',', 'PUNCT'), ('and', 'CCONJ'), ('I', 'PRON'), ('revised', 'VERB'), ('the', 'DET'), ('article', 'NOUN'), ('accordingly', 'ADV')]\n",
      "[('In', 'ADP'), ('organizations', 'NOUN'), (',', 'PUNCT'), ('tasks', 'NOUN'), ('like', 'ADP'), ('this', 'PRON'), ('can', 'AUX'), ('assist', 'VERB'), ('strategic', 'ADJ'), ('thinking', 'NOUN'), ('or', 'CCONJ'), ('scenario', 'NOUN'), ('-', 'PUNCT'), ('planning', 'NOUN'), ('exercises', 'NOUN')]\n",
      "[('Although', 'SCONJ'), ('there', 'PRON'), ('is', 'VERB'), ('tremendous', 'ADJ'), ('potential', 'NOUN'), ('for', 'ADP'), ('such', 'ADJ'), ('applications', 'NOUN'), (',', 'PUNCT'), ('right', 'ADV'), ('now', 'ADV'), ('the', 'DET'), ('results', 'NOUN'), ('are', 'AUX'), ('still', 'ADV'), ('relatively', 'ADV'), ('crude', 'ADJ'), (',', 'PUNCT'), ('but', 'CCONJ'), ('they', 'PRON'), ('can', 'AUX'), ('already', 'ADV'), ('add', 'VERB'), ('value', 'NOUN'), ('in', 'ADP'), ('their', 'PRON'), ('current', 'ADJ'), ('state', 'NOUN')]\n",
      "[('The', 'DET'), ('bottom', 'ADJ'), ('line', 'NOUN'), ('is', 'AUX'), ('that', 'SCONJ'), ('you', 'PRON'), ('need', 'VERB'), ('to', 'PART'), ('encourage', 'VERB'), ('broad', 'ADJ'), ('adoption', 'NOUN'), ('of', 'ADP'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('tools', 'NOUN'), ('throughout', 'ADP'), ('your', 'PRON'), ('business', 'NOUN')]\n",
      "[('It', 'PRON'), ('is', 'AUX'), ('difficult', 'ADJ'), ('to', 'PART'), ('anticipate', 'VERB'), ('just', 'ADV'), ('how', 'SCONJ'), ('these', 'DET'), ('tools', 'NOUN'), ('might', 'AUX'), ('be', 'AUX'), ('used', 'VERB'), ('at', 'ADP'), ('different', 'ADJ'), ('levels', 'NOUN'), ('of', 'ADP'), ('your', 'PRON'), ('organization', 'NOUN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('the', 'DET'), ('best', 'ADJ'), ('way', 'NOUN'), ('to', 'PART'), ('get', 'VERB'), ('an', 'DET'), ('understanding', 'NOUN'), ('of', 'ADP'), ('this', 'DET'), ('tech', 'NOUN'), ('may', 'AUX'), ('be', 'AUX'), ('for', 'ADP'), ('you', 'PRON'), ('and', 'CCONJ'), ('other', 'ADJ'), ('leaders', 'NOUN'), ('in', 'ADP'), ('your', 'PRON'), ('firm', 'NOUN'), ('to', 'PART'), ('adopt', 'VERB'), ('it', 'PRON'), ('yourselves', 'NOUN')]\n",
      "[('Don', 'PROPN'), ('’', 'PUNCT'), ('t', 'NOUN'), ('bet', 'VERB'), ('the', 'DET'), ('boat', 'NOUN'), ('on', 'ADP'), ('it', 'PRON'), ('because', 'SCONJ'), ('some', 'PRON'), ('of', 'ADP'), ('the', 'DET'), ('tech', 'NOUN'), ('may', 'AUX'), ('not', 'PART'), ('work', 'VERB'), ('out', 'ADP'), (',', 'PUNCT'), ('but', 'CCONJ'), ('if', 'SCONJ'), ('your', 'PRON'), ('team', 'NOUN'), ('gains', 'VERB'), ('a', 'DET'), ('better', 'ADJ'), ('understanding', 'NOUN'), ('of', 'ADP'), ('what', 'PRON'), ('is', 'AUX'), ('possible', 'ADJ'), (',', 'PUNCT'), ('then', 'ADV'), ('you', 'PRON'), ('will', 'AUX'), ('be', 'AUX'), ('ahead', 'ADV'), ('of', 'ADP'), ('the', 'DET'), ('competition', 'NOUN')]\n",
      "[('Remember', 'VERB'), ('that', 'SCONJ'), ('while', 'SCONJ'), ('current', 'ADJ'), ('AI', 'PROPN'), ('might', 'AUX'), ('not', 'PART'), ('be', 'AUX'), ('poised', 'VERB'), ('to', 'PART'), ('replace', 'VERB'), ('managers', 'NOUN'), (',', 'PUNCT'), ('managers', 'NOUN'), ('who', 'PRON'), ('understand', 'VERB'), ('AI', 'PROPN'), ('are', 'AUX'), ('poised', 'VERB'), ('to', 'PART'), ('replace', 'VERB'), ('managers', 'NOUN'), ('who', 'PRON'), ('don', 'VERB'), ('’', 'PUNCT'), ('t', 'NOUN')]\n",
      "[('Do', 'AUX'), ('not', 'PART'), ('underestimate', 'VERB'), ('the', 'DET'), ('transformative', 'ADJ'), ('potential', 'NOUN'), ('of', 'ADP'), ('AI', 'PROPN')]\n",
      "[('Large', 'ADJ'), ('foundation', 'NOUN'), ('models', 'NOUN'), ('like', 'ADP'), ('GPT-3', 'PROPN'), ('exhibit', 'NOUN'), ('abilities', 'NOUN'), ('to', 'PART'), ('generalize', 'VERB'), ('to', 'ADP'), ('a', 'DET'), ('large', 'ADJ'), ('number', 'NOUN'), ('of', 'ADP'), ('tasks', 'NOUN'), ('without', 'ADP'), ('any', 'DET'), ('task', 'NOUN'), ('-', 'PUNCT'), ('specific', 'ADJ'), ('training', 'NOUN')]\n",
      "[('The', 'DET'), ('recent', 'ADJ'), ('progress', 'NOUN'), ('in', 'ADP'), ('this', 'DET'), ('tech', 'NOUN'), ('is', 'AUX'), ('a', 'DET'), ('significant', 'ADJ'), ('step', 'NOUN'), ('toward', 'ADP'), ('human', 'ADJ'), ('-', 'PUNCT'), ('level', 'NOUN'), ('generalization', 'NOUN'), ('and', 'CCONJ'), ('general', 'ADJ'), ('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('that', 'PRON'), ('are', 'AUX'), ('the', 'DET'), ('ultimate', 'ADJ'), ('goals', 'NOUN'), ('of', 'ADP'), ('many', 'ADJ'), ('AI', 'PROPN'), ('researchers', 'NOUN'), (',', 'PUNCT'), ('including', 'VERB'), ('those', 'PRON'), ('at', 'ADP'), ('OpenAI', 'PROPN'), ('and', 'CCONJ'), ('Google', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('DeepMind', 'PROPN')]\n",
      "[('Such', 'ADJ'), ('systems', 'NOUN'), ('have', 'VERB'), ('tremendous', 'ADJ'), ('disruptive', 'ADJ'), ('potential', 'NOUN'), ('that', 'PRON'), ('could', 'AUX'), ('lead', 'VERB'), ('to', 'ADP'), ('AI', 'PROPN'), ('-', 'PUNCT'), ('driven', 'VERB'), ('explosive', 'ADJ'), ('economic', 'ADJ'), ('growth', 'NOUN'), (',', 'PUNCT'), ('which', 'PRON'), ('would', 'AUX'), ('radically', 'ADV'), ('transform', 'VERB'), ('business', 'NOUN'), ('and', 'CCONJ'), ('society', 'NOUN')]\n",
      "[('While', 'SCONJ'), ('you', 'PRON'), ('may', 'AUX'), ('still', 'ADV'), ('be', 'AUX'), ('skeptical', 'ADJ'), ('of', 'ADP'), ('radically', 'ADV'), ('transformative', 'VERB'), ('AI', 'PROPN'), ('like', 'ADP'), ('artificial', 'ADJ'), ('general', 'ADJ'), ('intelligence', 'NOUN'), (',', 'PUNCT'), ('it', 'PRON'), ('is', 'AUX'), ('prudent', 'ADJ'), ('for', 'SCONJ'), ('organizations', 'NOUN'), ('’', 'PART'), ('leaders', 'NOUN'), ('to', 'PART'), ('be', 'AUX'), ('cognizant', 'ADJ'), ('of', 'ADP'), ('early', 'ADJ'), ('signs', 'NOUN'), ('of', 'ADP'), ('progress', 'NOUN'), ('due', 'ADP'), ('to', 'ADP'), ('its', 'PRON'), ('tremendous', 'ADJ'), ('disruptive', 'ADJ'), ('potential', 'NOUN')]\n",
      "[('Consider', 'VERB'), ('that', 'SCONJ'), ('former', 'ADJ'), ('Google', 'PROPN'), ('chief', 'NOUN'), ('Eric', 'PROPN'), ('Schmidt', 'PROPN'), ('expects', 'VERB'), ('general', 'ADJ'), ('artificial', 'ADJ'), ('intelligence', 'NOUN'), ('in', 'ADP'), ('10–20', 'NUM'), ('years', 'NOUN'), ('and', 'CCONJ'), ('that', 'SCONJ'), ('the', 'DET'), ('UK', 'PROPN'), ('recently', 'ADV'), ('took', 'VERB'), ('an', 'DET'), ('official', 'ADJ'), ('position', 'NOUN'), ('on', 'ADP'), ('risks', 'NOUN'), ('from', 'ADP'), ('artificial', 'ADJ'), ('general', 'ADJ'), ('intelligence', 'NOUN')]\n",
      "[('Had', 'AUX'), ('organizations', 'NOUN'), ('paid', 'VERB'), ('attention', 'NOUN'), ('to', 'ADP'), ('Anthony', 'PROPN'), ('Fauci', 'PROPN'), ('’', 'PUNCT'), ('s', 'PART'), ('2017', 'NUM'), ('warning', 'NOUN'), ('on', 'ADP'), ('the', 'DET'), ('importance', 'NOUN'), ('of', 'ADP'), ('pandemic', 'ADJ'), ('preparedness', 'NOUN'), (',', 'PUNCT'), ('the', 'DET'), ('most', 'ADV'), ('severe', 'ADJ'), ('effects', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('pandemic', 'ADJ'), ('and', 'CCONJ'), ('ensuing', 'VERB'), ('supply', 'NOUN'), ('chain', 'NOUN'), ('crisis', 'NOUN'), ('may', 'AUX'), ('have', 'AUX'), ('been', 'AUX'), ('avoided', 'VERB')]\n",
      "[('Ignoring', 'VERB'), ('the', 'DET'), ('transformative', 'ADJ'), ('potential', 'NOUN'), ('of', 'ADP'), ('AI', 'PROPN'), ('also', 'ADV'), ('carries', 'VERB'), ('risks', 'NOUN'), (',', 'PUNCT'), ('and', 'CCONJ'), ('similar', 'ADJ'), ('to', 'ADP'), ('the', 'DET'), ('supply', 'NOUN'), ('chain', 'NOUN'), ('crisis', 'NOUN'), (',', 'PUNCT'), ('firms', 'NOUN'), ('’', 'PART'), ('inaction', 'NOUN'), ('or', 'CCONJ'), ('irresponsible', 'ADJ'), ('use', 'NOUN'), ('of', 'ADP'), ('AI', 'PROPN'), ('could', 'AUX'), ('have', 'VERB'), ('widespread', 'ADJ'), ('and', 'CCONJ'), ('damaging', 'ADJ'), ('effects', 'NOUN'), ('on', 'ADP'), ('society', 'NOUN'), ('(', 'PUNCT'), ('e', 'PROPN')]\n",
      "[('g', 'X')]\n",
      "[(',', 'PUNCT'), ('increasing', 'VERB'), ('inequality', 'NOUN'), ('or', 'CCONJ'), ('domain', 'NOUN'), ('-', 'PUNCT'), ('specific', 'ADJ'), ('risks', 'NOUN'), ('from', 'ADP'), ('automation', 'NOUN'), (')', 'PUNCT')]\n",
      "[('However', 'ADV'), (',', 'PUNCT'), ('unlike', 'ADP'), ('the', 'DET'), ('supply', 'NOUN'), ('chain', 'NOUN'), ('crisis', 'NOUN'), (',', 'PUNCT'), ('societal', 'ADJ'), ('changes', 'NOUN'), ('from', 'ADP'), ('transformative', 'ADJ'), ('AI', 'PROPN'), ('will', 'AUX'), ('likely', 'ADV'), ('be', 'AUX'), ('irreversible', 'ADJ'), ('and', 'CCONJ'), ('could', 'AUX'), ('even', 'ADV'), ('continue', 'VERB'), ('to', 'PART'), ('accelerate', 'VERB')]\n",
      "[('Organizations', 'NOUN'), ('should', 'AUX'), ('begin', 'VERB'), ('preparing', 'VERB'), ('now', 'ADV'), ('not', 'PART'), ('only', 'ADV'), ('to', 'PART'), ('capitalize', 'VERB'), ('on', 'ADP'), ('transformative', 'ADJ'), ('AI', 'PROPN'), (',', 'PUNCT'), ('but', 'CCONJ'), ('to', 'PART'), ('do', 'VERB'), ('their', 'PRON'), ('part', 'NOUN'), ('to', 'PART'), ('avoid', 'VERB'), ('undesirable', 'ADJ'), ('futures', 'NOUN'), ('and', 'CCONJ'), ('ensure', 'VERB'), ('that', 'SCONJ'), ('advanced', 'ADJ'), ('AI', 'PROPN'), ('is', 'AUX'), ('used', 'VERB'), ('to', 'PART'), ('equitably', 'ADV'), ('benefit', 'VERB'), ('society', 'NOUN')]\n",
      "[('Language', 'NOUN'), ('-', 'PUNCT'), ('Based', 'VERB'), ('AI', 'PROPN'), ('Tools', 'PROPN'), ('Are', 'AUX'), ('Here', 'ADV'), ('to', 'PART'), ('Stay', 'VERB')]\n",
      "[('Powerful', 'ADJ'), ('generalizable', 'ADJ'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('tools', 'NOUN'), ('like', 'ADP'), ('Elicit', 'PROPN'), ('are', 'AUX'), ('here', 'ADV'), (',', 'PUNCT'), ('and', 'CCONJ'), ('they', 'PRON'), ('are', 'AUX'), ('just', 'ADV'), ('the', 'DET'), ('tip', 'NOUN'), ('of', 'ADP'), ('the', 'DET'), ('iceberg', 'NOUN'), (';', 'PUNCT'), ('multimodal', 'NOUN'), ('foundation', 'NOUN'), ('model', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('tools', 'NOUN'), ('are', 'AUX'), ('poised', 'VERB'), ('to', 'PART'), ('transform', 'VERB'), ('business', 'NOUN'), ('in', 'ADP'), ('ways', 'NOUN'), ('that', 'PRON'), ('are', 'AUX'), ('still', 'ADV'), ('difficult', 'ADJ'), ('to', 'PART'), ('predict', 'VERB')]\n",
      "[('To', 'PART'), ('begin', 'VERB'), ('preparing', 'VERB'), ('now', 'ADV'), (',', 'PUNCT'), ('start', 'VERB'), ('understanding', 'VERB'), ('your', 'PRON'), ('text', 'NOUN'), ('data', 'NOUN'), ('assets', 'NOUN'), ('and', 'CCONJ'), ('the', 'DET'), ('variety', 'NOUN'), ('of', 'ADP'), ('cognitive', 'ADJ'), ('tasks', 'NOUN'), ('involved', 'VERB'), ('in', 'ADP'), ('different', 'ADJ'), ('roles', 'NOUN'), ('in', 'ADP'), ('your', 'PRON'), ('organization', 'NOUN')]\n",
      "[('Aggressively', 'ADV'), ('adopt', 'VERB'), ('new', 'ADJ'), ('language', 'NOUN'), ('-', 'PUNCT'), ('based', 'VERB'), ('AI', 'PROPN'), ('technologies', 'NOUN'), (';', 'PUNCT'), ('some', 'PRON'), ('will', 'AUX'), ('work', 'VERB'), ('well', 'ADV'), ('and', 'CCONJ'), ('others', 'NOUN'), ('will', 'AUX'), ('not', 'PART'), (',', 'PUNCT'), ('but', 'CCONJ'), ('your', 'PRON'), ('employees', 'NOUN'), ('will', 'AUX'), ('be', 'AUX'), ('quicker', 'ADJ'), ('to', 'PART'), ('adjust', 'VERB'), ('when', 'SCONJ'), ('you', 'PRON'), ('move', 'VERB'), ('on', 'ADP'), ('to', 'ADP'), ('the', 'DET'), ('next', 'ADJ')]\n",
      "[('And', 'CCONJ'), ('don', 'PROPN'), ('’', 'PUNCT'), ('t', 'NOUN'), ('forget', 'VERB'), ('to', 'PART'), ('adopt', 'VERB'), ('these', 'DET'), ('technologies', 'NOUN'), ('yourself', 'PRON'), ('—', 'PUNCT'), ('this', 'PRON'), ('is', 'AUX'), ('the', 'DET'), ('best', 'ADJ'), ('way', 'NOUN'), ('for', 'SCONJ'), ('you', 'PRON'), ('to', 'PART'), ('start', 'VERB'), ('to', 'PART'), ('understand', 'VERB'), ('their', 'PRON'), ('future', 'ADJ'), ('roles', 'NOUN'), ('in', 'ADP'), ('your', 'PRON'), ('organization', 'NOUN')]\n",
      "The most concerete sentence:  Large foundation models like GPT-3 exhibit abilities to generalize to a large number of tasks without any task-specific training, 0.450\n",
      "\n",
      "The least concerete sentence:  What NLP Can Do, 0.000\n"
     ]
    }
   ],
   "source": [
    "# Find the most concrete and the least concrete sentences from the article\n",
    "\n",
    "concrete = [compute_concreteness(x)[0] for x in tokenized_sents]\n",
    "max_id = np.argmax(np.array(concrete))\n",
    "min_id = np.argmin(np.array(concrete))\n",
    "print (f\"The most concerete sentence:  {sents[max_id]}, {concrete[max_id]:.3f}\\n\")\n",
    "print (f\"The least concerete sentence:  {sents[min_id]}, {concrete[min_id]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SFBMtpV7-4KM"
   },
   "source": [
    "### Q2.3. Generate TF-IDF representations for sentences (1 point,  0.5 point for use_idf option, 0.5 point for overall)\n",
    "\n",
    "Define a function `compute_tf_idf(sents, use_idf)` as follows: \n",
    "\n",
    "\n",
    "- Take the following two inputs:\n",
    "    - `sents`: tokenized sentences returned from Q2.1. These sentences form a corpus for you to calculate `TF-IDF` vectors.\n",
    "    - `use_idf`: if this option is true, return smoothed normalized `TF_IDF` vectors for all sentences; otherwise, just return normalized `TF` vector for each sentence.\n",
    "    \n",
    "    \n",
    "- Calculate `TF-IDF` vectors as shown in the lecture notes (Hint: you can slightly modify code segment 7.5 in NLP Lecture Notes (II) for this task)\n",
    "\n",
    "- Return the `TF-IDF` vectors  if `use_idf` is True.  Return the `TF` vectors if `use_idf` is False."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk, re, string\n",
    "from sklearn.preprocessing import normalize\n",
    "from nltk.corpus import stopwords\n",
    "import numpy as np  \n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "def compute_tf_idf(sents, use_idf = True, min_df = 1):\n",
    "    \n",
    "    if use_idf:\n",
    "    # step 2. process all documents to get list of token list\n",
    "        docs_tokens={idx:get_doc_tokens(doc) for idx,doc in enumerate(docs)}\n",
    "\n",
    "    # step 3. get document-term matrix\n",
    "        dtm=pd.DataFrame.from_dict(docs_tokens, orient=\"index\" )\n",
    "        dtm=dtm.fillna(0)\n",
    "        dtm = dtm.sort_index(axis = 0)\n",
    "      \n",
    "    # step 4. get normalized term frequency (tf) matrix        \n",
    "        tf=dtm.values\n",
    "        doc_len=tf.sum(axis=1, keepdims=True)\n",
    "        tf=np.divide(tf, doc_len)\n",
    "    \n",
    "    # step 5. get idf\n",
    "        df=np.where(tf>0,1,0)\n",
    "    #idf=np.log(np.divide(len(docs), \\\n",
    "    #    np.sum(df, axis=0)))+1\n",
    "\n",
    "        smoothed_idf=np.log(np.divide(len(docs)+1, np.sum(df, axis=0)+1))+1    \n",
    "        smoothed_tf_idf=tf*smoothed_idf\n",
    "    \n",
    "    return smoothed_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:27:57.344401Z",
     "start_time": "2021-10-19T07:27:56.848401Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OBOzkpEk-4KN",
    "outputId": "8a38a63c-b55c-44f5-b2bf-a3b3b5a7d4c0"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [34]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m sents, tokenized_sents \u001b[38;5;241m=\u001b[39m preprocess(doc)\n\u001b[0;32m----> 2\u001b[0m tf_idf \u001b[38;5;241m=\u001b[39m \u001b[43mcompute_tf_idf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtokenized_sents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_idf\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# show shape of TF-IDF\u001b[39;00m\n\u001b[1;32m      5\u001b[0m tf_idf\u001b[38;5;241m.\u001b[39mshape\n",
      "Input \u001b[0;32mIn [33]\u001b[0m, in \u001b[0;36mcompute_tf_idf\u001b[0;34m(sents, use_idf, min_df)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_tf_idf\u001b[39m(sents, use_idf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m, min_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_idf:\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;66;03m# step 2. process all documents to get list of token list\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m         docs_tokens\u001b[38;5;241m=\u001b[39m{idx:get_doc_tokens(doc) \u001b[38;5;28;01mfor\u001b[39;00m idx,doc \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mdocs\u001b[49m)}\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# step 3. get document-term matrix\u001b[39;00m\n\u001b[1;32m     15\u001b[0m         dtm\u001b[38;5;241m=\u001b[39mpd\u001b[38;5;241m.\u001b[39mDataFrame\u001b[38;5;241m.\u001b[39mfrom_dict(docs_tokens, orient\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mindex\u001b[39m\u001b[38;5;124m\"\u001b[39m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'docs' is not defined"
     ]
    }
   ],
   "source": [
    "sents, tokenized_sents = preprocess(doc)\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "\n",
    "# show shape of TF-IDF\n",
    "tf_idf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mEYffajS-4KN"
   },
   "source": [
    "### Q2.4. Identify key sentences as summary \n",
    "\n",
    "`2 points, 0.5 point for steps 1-3 each, 0.5 for overall logic. Due ot different packages used, the output summary sentences may not be the same as the sample output. If different, please check the code to check if the coding logic is correct`\n",
    "\n",
    "The basic idea is that, in a coherence article, all sentences should center around some key ideas. If we can identify a subset of sentences, denoted as $S_{key}$, which precisely capture the key ideas,  then $S_{key}$ can be used as a summary. Moreover, $S_{key}$ should have high similarity to all the other sentences on average, because all sentences are centered around the key ideas contained in $S_{key}$. Therefore, we can identify whether a sentence belongs to $S_{key}$ by its similarity to all the other sentences.\n",
    "\n",
    "\n",
    "Define a function `get_summary(tf_idf, sents, topN = 5)`  as follows:\n",
    "\n",
    "- This function takes three inputs:\n",
    "    - `tf_idf`: the TF-IDF vectors of all the sentences in a document\n",
    "    - `sents`: the original sentences corresponding to the TF-IDF vectors\n",
    "    - `topN`: the top N sentences in the generated summary\n",
    "\n",
    "- Steps:\n",
    "    1. Calculate the cosine similarity for every pair of TF-IDF vectors (0.5 point)\n",
    "    1. For each sentence, calculate its average similarity to all the others (0.5 point)\n",
    "    1. Select the sentences with the `topN` largest average similarity (0.5 point)\n",
    "    1. Print the `topN` sentences index\n",
    "    1. Return these sentences as the summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:47.949165Z",
     "start_time": "2021-10-19T07:29:47.945086Z"
    },
    "id": "gDqgTk8c-4KO"
   },
   "outputs": [],
   "source": [
    "def get_summary(tf_idf, sents, topN = 5):\n",
    "    \n",
    "   #add your codes\n",
    "\n",
    "\n",
    "    return summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-10-19T07:29:50.689152Z",
     "start_time": "2021-10-19T07:29:48.264799Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sw5L48aK-4KO",
    "outputId": "1d5109ef-432f-4cf5-d0b6-46d1b08dc96e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Begin incorporating new language-based AI tools for a variety of tasks to better understand their capabilities.. \n",
      "\n",
      "Powerful generalizable language-based AI tools like Elicit are here, and they are just the tip of the iceberg; multimodal foundation model-based tools are poised to transform business in ways that are still difficult to predict. \n",
      "\n",
      "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
      "\n",
      "There is so much text data, and you don’t need advanced models like GPT-3 to extract its value. \n",
      "\n",
      "Understand how you might leverage AI-based language technologies to make better decisions or reorganize your skilled labor.. Language-based AI won’t replace jobs, but it will automate many tasks, even for decision makers. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# put everything together and test with different options\n",
    "\n",
    "sents, tokenized_sents = preprocess(doc)\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "summary = get_summary(tf_idf, sents, topN = 5)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
      "\n",
      "It is difficult to anticipate just how these tools might be used at different levels of your organization, but the best way to get an understanding of this tech may be for you and other leaders in your firm to adopt it yourselves. \n",
      "\n",
      "Identify your text data assets and determine how the latest techniques can be leveraged to add value for your firm.. You are certainly aware of the value of data, but you still may be overlooking some essential data assets if you are not utilizing text analytics and NLP throughout your organization. \n",
      "\n",
      "These new tools will transcend traditional business intelligence and will transform the nature of many roles in organizations — programmers are just the beginning.. You need to start understanding how these technologies can be used to reorganize your skilled labor. \n",
      "\n",
      "Powerful generalizable language-based AI tools like Elicit are here, and they are just the tip of the iceberg; multimodal foundation model-based tools are poised to transform business in ways that are still difficult to predict. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with the option lemmatized=False, remove_stopword=False\n",
    "\n",
    "sents, tokenized_sents = preprocess(text, lemmatized=False, remove_stopword=False, remove_punctuation = True )\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = True)\n",
    "summary = get_summary(tf_idf, sents, topN = 5)\n",
    "for sent in summary:\n",
    "   print(sent,\"\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Powerful generalizable language-based AI tools like Elicit are here, and they are just the tip of the iceberg; multimodal foundation model-based tools are poised to transform business in ways that are still difficult to predict. \n",
      "\n",
      "Begin incorporating new language-based AI tools for a variety of tasks to better understand their capabilities.. \n",
      "\n",
      "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
      "\n",
      "Understand how you might leverage AI-based language technologies to make better decisions or reorganize your skilled labor.. Language-based AI won’t replace jobs, but it will automate many tasks, even for decision makers. \n",
      "\n",
      "NLP practitioners call tools like this “language models,” and they can be used for simple analytics tasks, such as classifying documents and analyzing the sentiment in blocks of text, as well as more advanced tasks, such as answering questions and summarizing reports. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# test with the option use_idf = False\n",
    "\n",
    "sents, tokenized_sents = preprocess(text)\n",
    "tf_idf = compute_tf_idf(tokenized_sents, use_idf = False)\n",
    "summary = get_summary(tf_idf, sents, topN = 5)\n",
    "for sent in summary:\n",
    "   print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4l0bXLQG-4KO"
   },
   "source": [
    "### Q2.5. Analysis (1 point, 0.5 point for Q1, and 0.5 for all the others)\n",
    "\n",
    "- Do you think the way to quantify concreteness makes sense? Any other thoughts to measure concreteness or abstractness? Share your ideas in pdf or markdown.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Do you think this method is able to generate a good summary? Any pros or cons have you observed? (0.5 point)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Do these options `lemmatized, remove_stopword, remove_punctuation, use_idf` matter? \n",
    "- Why do you think these options matter or do not matter? \n",
    "- If these options matter, what are the best values for these options?\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It kind of returns a grammatically sound output.\n",
    "We can test accuracy of correctness by making a new function and testing our corrected output with say a predicted output\n",
    "\n",
    "\n",
    "lemmatized, remove_stopword, remove_punctuation, use_idf matter - do matter, helps filter out\n",
    "lemmatize will give the root/lemma of the word so any form of the word(adj,verb etc) will be catchable\n",
    "punctuation removal is important too to strip punctuations, stowards removes general stopwords( common words that we do not need. We can keep updating stopwords dict\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZIyY8lIr-4KP"
   },
   "source": [
    "## Q2.5. (Bonus 1 point). \n",
    "\n",
    "`While the idea is proposed, it must be implemented. `\n",
    "\n",
    "\n",
    "- Can you think a way to improve this extractive summary method? Explain the method you propose for improvement,  implement it, use it to generate a new summary, and demonstrate what is improved in the new summary.\n",
    "\n",
    "**Sample Answer**\n",
    "\n",
    "\n",
    "A: If an article have sentences repeats themselves and if one of the repeated sentences is selected, the other will be selected too. It is hard to ensure the diverity of the sentences in the summary.  To ensure diversity, tor example, this algorithm can be improved using **max-min** method: \n",
    "1. Select top 10 (or more) sentences as before as candidates. \n",
    "1. Add top 1 from the candidates into the summary, \n",
    "1. gradually add other sentences such that each of them is **least similar** to those aleady added. \n",
    "An implementation is provided. See if it's better!\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- Or, you can research on some other extractive summary methods and implement one here. Compare it with the one you implemented in Q2.1-Q2.3 and show pros and cons of each method.\n",
    "\n",
    "*Another alogithm can be selecting the sentences which have the largest total word tf-idf scores. For implementation, see https://towardsdatascience.com/text-summarization-using-tf-idf-e64a0644ace3*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_summary_with_diversity(tf_idf, sents, topN = 5):\n",
    "    \n",
    "   \n",
    "    #add your codes\n",
    "\n",
    "\n",
    "\n",
    "    return summary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP practitioners call tools like this “language models,” and they can be used for simple analytics tasks, such as classifying documents and analyzing the sentiment in blocks of text, as well as more advanced tasks, such as answering questions and summarizing reports. \n",
      "\n",
      "This transformative capability was already expected to change the nature of how programmers do their jobs, but models continue to improve — the latest from Google’s DeepMind AI lab, for example, demonstrates the critical thinking and logic skills necessary to outperform most humans in programming competitions.. Models like GPT-3 are considered to be foundation models — an emerging AI research area — which also work for other types of data such as images and video. \n",
      "\n",
      "There is so much text data, and you don’t need advanced models like GPT-3 to extract its value. \n",
      "\n",
      "Understand how you might leverage AI-based language technologies to make better decisions or reorganize your skilled labor.. Language-based AI won’t replace jobs, but it will automate many tasks, even for decision makers. \n",
      "\n",
      "Powerful generalizable language-based AI tools like Elicit are here, and they are just the tip of the iceberg; multimodal foundation model-based tools are poised to transform business in ways that are still difficult to predict. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "summary = get_summary_with_diversity(tf_idf, sents, topN = 5)\n",
    "\n",
    "for sent in summary:\n",
    "    print(sent,\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "HW_4.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
